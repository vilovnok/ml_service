{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from transformers import AutoTokenizer\n",
    "from langchain_gigachat.embeddings import GigaChatEmbeddings\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "embedder = GigaChatEmbeddings()\n",
    "tokenizer = AutoTokenizer.from_pretrained('r1char9/rubert-base-cased-russian-sentiment')\n",
    "\n",
    "# vectorstore = FAISS.from_texts(['create faiss text example'],\n",
    "#                                embedding=embedder,\n",
    "#                                normalize_L2=True,\n",
    "#                                verify=False\n",
    "#                                )\n",
    "\n",
    "\n",
    "text = \"\"\"\n",
    "Конечно! Вот пример длинного документа на русском языке. Это может быть, например, текст статьи, эссе или технического руководства. Я создам структурированный документ с заголовками, подзаголовками и разделами.\n",
    "**Заголовок: Влияние технологий на современное общество**\n",
    "**Введение**  \n",
    "Современное общество находится в постоянном движении, и одним из главных катализаторов этого движения являются технологии. Начиная с изобретения колеса и заканчивая искусственным интеллектом, технологии всегда играли ключевую роль в развитии человечества. В этой статье мы рассмотрим, как современные технологии влияют на различные аспекты нашей жизни: образование, работу, общение и культуру.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length_tokenizer(tokenizer):\n",
    "    tokens = tokenizer.tokenize(tokenizer)\n",
    "    return len(tokens)\n",
    "\n",
    "child_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=32,\n",
    "    length_function=length_tokenizer\n",
    ")\n",
    "parent_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2048,\n",
    "    chunk_overlap=256,\n",
    "    length_function=length_tokenizer\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=\n",
    ")\n",
    "\n",
    "\n",
    "big_doc_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=16384,\n",
    "    chunk_overlap=256,\n",
    "    length_function=length_tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('r1char9/rubert-base-cased-russian-sentiment')\n",
    "\n",
    "\n",
    "text = \"ArithmeticError: unexpected 123 : [123 123], 123``123\"\n",
    "\n",
    "def token_length_function(text: str) -> int:    \n",
    "    encoded_text = tokenizer.tokenize(text, add_special_tokens=False)\n",
    "    \n",
    "    return len(encoded_text)\n",
    "\n",
    "\n",
    "debug_child_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=20,\n",
    "    chunk_overlap=5,\n",
    "    length_function=token_length_function\n",
    ")\n",
    "\n",
    "pop = debug_child_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.0\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "def token_length_function(text: str) -> int:\n",
    "    encoded_text = tokenizer.tokenize(text, add_special_tokens=False)\n",
    "    \n",
    "    length_encode = len(encoded_text)\n",
    "    length_symbole = len(text) / 3\n",
    "    \n",
    "    print(length_symbole)\n",
    "    print(length_encode)\n",
    "\n",
    "r = 'Привет как дела что тебе надо и зачем ты мне пишешь'\n",
    "token_length_function(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "\n",
    "task_name_to_instruct = {\n",
    "    \"example\": \"Дайте ответ на вопрос, найдя соответствующие отрывки текста.\",\n",
    "}\n",
    "\n",
    "query_prefix = task_name_to_instruct[\"example\"] + \"\\nquestion: \"\n",
    "queries = [\n",
    "    'Разрешены ли броски из дзюдо в борьбе?', \n",
    "    'Как стать рентгенологом в Мичигане?'\n",
    "]\n",
    "\n",
    "passage_prefix = \"\"\n",
    "passages = [\n",
    "    \"Если вы читаете это, вы, вероятно, занимаетесь дзюдо или просто интересуетесь, как техники дзюдо могут применяться в борьбе. Итак, без лишних слов, перейдем к вопросу. Разрешены ли броски из дзюдо в борьбе? Да, броски из дзюдо разрешены в вольной и народной борьбе. Вам нужно только быть осторожным, чтобы соблюдать правила борьбы при выполнении бросков. В борьбе запрещено бросать соперника на мат с чрезмерной силой.\",\n",
    "    \"Ниже приведены основные шаги, чтобы стать рентгенологом в Мичигане: Получите аттестат о среднем образовании. Как и в большинстве профессий в сфере здравоохранения, среднее образование — это первый шаг к получению работы начального уровня. Изучение математики и естественных наук, таких как анатомия, биология, химия, физиология и физика, может помочь подготовиться к учебе в колледже и будущей карьере. Получите степень младшего специалиста. Для начальных позиций в рентгенологии обычно требуется как минимум степень младшего специалиста. Перед поступлением в одну из таких программ убедитесь, что она аккредитована Объединенным комитетом по образованию в области радиологических технологий (JRCERT). Получите лицензию или сертификат в штате Мичиган.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4ec103a86e949d28bd21b11ec7ef0a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained('ai-sage/Giga-Embeddings-instruct', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embeddings = model.encode(queries, instruction=query_prefix)\n",
    "passage_embeddings = model.encode(passages, instruction=passage_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[69.92779541015625, 13.716340065002441], [16.950559616088867, 67.88148498535156]]\n"
     ]
    }
   ],
   "source": [
    "query_embeddings = F.normalize(query_embeddings, p=2, dim=1)\n",
    "passage_embeddings = F.normalize(passage_embeddings, p=2, dim=1)\n",
    "\n",
    "scores = (query_embeddings @ passage_embeddings.T) * 100\n",
    "print(scores.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "Module langchain_community.vectorstores not found. Please install langchain-community to access this module. You can install it using `pip install -U langchain-community`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain/_api/module_import.py:69\u001b[0m, in \u001b[0;36mcreate_importer.<locals>.import_by_name\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 69\u001b[0m     module \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(new_module)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1204\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1126\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1204\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1140\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain_community'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstores\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FAISS\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstorage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InMemoryStore\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext_splitter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain/vectorstores/__init__.py:186\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    185\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Look up attributes dynamically.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _import_attribute(name)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain/_api/module_import.py:72\u001b[0m, in \u001b[0;36mcreate_importer.<locals>.import_by_name\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m new_module\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlangchain_community\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 72\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(\n\u001b[1;32m     73\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_module\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     74\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install langchain-community to access this module. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     75\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can install it using `pip install -U langchain-community`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     76\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: Module langchain_community.vectorstores not found. Please install langchain-community to access this module. You can install it using `pip install -U langchain-community`"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "\n",
    "# Инициализация эмбеддингов\n",
    "embedder = HuggingFaceEmbeddings()\n",
    "\n",
    "# Создание векторного хранилища\n",
    "vectorstore = FAISS.from_texts(['create fauss text example'], embedder, normalize_l2=True)\n",
    "\n",
    "store = InMemoryStore()\n",
    "\n",
    "\n",
    "child_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,  \n",
    "    chunk_overlap=32,\n",
    "    length_function=lambda x: len(x))\n",
    "\n",
    "parent_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2048,  \n",
    "    chunk_overlap=256,\n",
    "    length_function=lambda x: len(x))\n",
    "\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter\n",
    ")\n",
    "\n",
    "big_doc_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=16384,\n",
    "    chunk_overlap=256,\n",
    "    length_function=lambda x: len(x))\n",
    "\n",
    "folder_docs = [...] \n",
    "\n",
    "for doc in folder_docs:\n",
    "    tokens = doc.page_content.split()\n",
    "    if len(tokens) > 20000:\n",
    "        sub_docs = big_doc_splitter.split_documents([doc])\n",
    "        for idx, sub_doc in enumerate(sub_docs):\n",
    "            sub_doc.metadata['part'] = idx\n",
    "            retriever.add_documents([sub_doc])\n",
    "    else:\n",
    "        retriever.add_documents([doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
